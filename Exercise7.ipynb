{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Negentropy is scale-invariant (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differential entropy of a n-dimensional random vector x with probability density p(x) is\n",
    "defined as   $$H(X)=-\\int_{\\Re^{2}}p(x)log p(x) dx$$ The negentropy is defined as $$J(X)=H(X_{Gauss})-H(X) $$ where $X_{Gauss}$ is a n-dimensional multivariate Gaussian random vector with the same covariance matrix as X. Show that the negentropy is invariant w.r.t invertible linear transformations $y=Ax,$ i.e. $$J(Ax)=J(x)$$ from which it follows that the negentropy is scale-invariant.$$$$ Use that the differential entropy of a multivariate n-dimensional Gaussian random vector x with covariance matrix $\\sum$ has the form $$H(X_Gauss)=\\frac{1}{2}log|det\\sum|+ \\frac{n}{2}(1+log 2\\pi )$$ Remarkï¼šthe differential entropy itself is not scale-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To show: $$J(AX)=J(X)$$ $$y=AX$$ Transformation of random variable $$p_{y}(y)=p_{x}(x){\\frac {1}{|det(J_y)|}} \\qquad J_y=A$$\n",
    "so $$p_{y}(y)=p_{x}(x){\\frac {1}{|det(A)|}} \\qquad (1)$$ $$$$\n",
    "$$H(y)=-\\int_{\\Re^{2}}p_{y}(y)log p_{y}(y) dy$$\n",
    "Insert (1) :$$H(y)=-\\int_{\\Re^{2}}\\frac{p_{x}(x)}{|det(A)|}log \\frac{p_{x}(x)}{|det(A)|}|det(A)| dx$$ $$$$ \n",
    "$$=-\\int_{\\Re^{2}}p_{x}(x)log \\frac{p_{x}(x)}{|det(A)|} dx$$ $$$$ \n",
    "$$=-\\int_{\\Re^{2}}p_{x}(x)log p_{x}(x)dx+log|det(A)|$$ $$$$  \n",
    "so $$H(AX)=H(y)=H(X)+log|det(A)| \\qquad (2) $$ $$$$ \n",
    "$$\\sum_{y}=E[AXX^TA^T]=E[AX]E[X^TA^T]=A\\sum_{x}A^T \\qquad (3)$$ $$$$\n",
    "$$ H(y_{Gauss})=\\frac{1}{2}log|det\\sum_{y}|+\\frac{n}{2}(1+log(2\\pi)) $$ $$$$\n",
    "insert (3):$$=\\frac{1}{2}log|det(A\\sum_{x}A^T)|+\\frac{n}{2}(1+log(2\\pi))$$ $$$$\n",
    "$$=\\frac{1}{2}log|det\\sum_{x}|+log(det(A))+\\frac{n}{2}(1+log(2\\pi)) \\qquad (4)$$\n",
    "$$J(AX)=J(y)=H(y_{Gauss})-H(y)$$ $$$$\n",
    "insert (2) (4):$$J(AX)=\\frac{1}{2}log|det\\sum_{x}|+log(det(A))+\\frac{n}{2}(1+log(2\\pi))-H(X)-log|det(A)|$$ $$$$\n",
    "$$=\\frac{1}{2}log|det\\sum_{x}|+\\frac{n}{2}(1+log(2\\pi))-H(X)$$ $$$$\n",
    "$$=H(X_{Gauss})-H(X)=J(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 7.2 fastICA vs. Infomax (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply fastICA to the two soundfiles data set (once again) of problem sheet 6 and compare\n",
    "runtime and robustness w.r.t. the mixing matrix A with the Infomax-based ICA-algorithm.\n",
    "Use the following setup for the latter algorithm: natural gradient, Bell-Sejnowski amplitude\n",
    "normalization, learning rate schedule \"0 = 0:01, \"t+1 = 0:9999\"t.\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = np.fromfile('sound1.dat', dtype=float, sep='\\n')\n",
    "s2 = np.fromfile('sound2.dat', dtype=float, sep='\\n')\n",
    "s=np.concatenate((s1[np.newaxis,:],s2[np.newaxis,:]),0)\n",
    "A = np.array([[4, 3], [2, 1]])  # Mixing matrix\n",
    "S=np.dot(A,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 7.3 ICA on Image Patches (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file imgpca.zip (used also in exercise sheet 2) contains three categories of images: nature,\n",
    "buildings, and text (prefixes n,b,t). For each category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Sample P patches of $\\sqrt{N}$ x  $\\sqrt{N}$ pixels from all images of this category and rearrange each sample to a column vector. Choose number and size of the patches according to your computing resources. Recommended are P $\\geq$ 20000 and N$\\geq$ 144."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26000L, 256L)\n",
      "(20000L, 256L)\n",
      "(28000L, 256L)\n"
     ]
    }
   ],
   "source": [
    "def getPatches(cat, n=10, patch_size=(16,16), patches_per_img=2000):\n",
    "    total_cols = patch_size[0]*patch_size[1]\n",
    "    matrix = np.zeros((n*patches_per_img, total_cols))\n",
    "    for i in range(n):\n",
    "        img = mpimg.imread('./imgpca/%s%d.jpg'%(cat, i+1))\n",
    "        str_idx = i*patches_per_img\n",
    "        patches = extract_patches_2d(img, patch_size=patch_size, max_patches = patches_per_img )\n",
    "        matrix[ str_idx:str_idx+patches_per_img, : ] = patches.reshape(patches_per_img, total_cols )\n",
    "    return matrix\n",
    "matrix_nature = getPatches('n',n=13)\n",
    "matrix_building = getPatches('b')\n",
    "matrix_text = getPatches('t',n=14)\n",
    "print(np.shape(matrix_nature))\n",
    "print(np.shape(matrix_building))\n",
    "print(np.shape(matrix_text))\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Calculate the independent features of the image patches (these are the columns of mixing matrix A). Use a fastICA toolbox to compute this matrix:\n",
    "1.Let fastica perform PCA and whitening of the data.\n",
    "2.Use the contrast function G($\\widehat{s}$) =$\\frac{1}{a}$ log cosh(a$\\widehat{s}$) with a = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Show the first 20 independent features as (grayscale) image patches by rearranging the vectors into$\\sqrt{N}$ x  $\\sqrt{N}$ matrices and compare the results for the different categories. Order the independent features by decreasing negentropy, (such that the first feature has largest (approximated) negentropy etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Perform PCA on the same set of patches, plot the the principal components (ordered by decreasing eigenvalue) as in (c) and compare them with the independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
